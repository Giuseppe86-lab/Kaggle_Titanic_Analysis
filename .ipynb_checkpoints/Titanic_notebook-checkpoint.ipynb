{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T20:41:59.772133Z","iopub.execute_input":"2024-10-31T20:41:59.773238Z","iopub.status.idle":"2024-10-31T20:42:00.213250Z","shell.execute_reply.started":"2024-10-31T20:41:59.773179Z","shell.execute_reply":"2024-10-31T20:42:00.212229Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\nprint(train_data[['Pclass','Cabin','Ticket']])","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:42:05.919105Z","iopub.execute_input":"2024-10-31T20:42:05.919622Z","iopub.status.idle":"2024-10-31T20:42:05.976444Z","shell.execute_reply.started":"2024-10-31T20:42:05.919584Z","shell.execute_reply":"2024-10-31T20:42:05.975446Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors = 5)\n\n# Calcolare il numero di passeggeri per ciascun porto e sopravvivenza\nembarked_survival_counts = train_data.groupby(['Embarked', 'Survived']).size().reset_index(name='Count')\n\n# Calcolare le percentuali rispetto al totale dei passeggeri imbarcati in ogni porto\nembarked_total_counts = train_data.groupby('Embarked')['PassengerId'].count().reset_index(name='Total')\nembarked_survival_counts = pd.merge(embarked_survival_counts, embarked_total_counts, on='Embarked')\nembarked_survival_counts['Percentage'] = (embarked_survival_counts['Count'] / embarked_survival_counts['Total']) * 100\n\n# Crea una griglia di 2 grafici su una riga\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\n# Creare un barplot per Embarked e Survived\nsns.countplot(data=train_data, x='Embarked', hue='Survived', palette='Set1', ax=axes[0])\n\n# Aggiungere titolo e etichette\naxes[0].set_title('Survival Count Based on Embarked Location')\naxes[0].set_xlabel('Port of Embarkation')\naxes[0].set_ylabel('Number of Passengers')\n\nsns.barplot(data=embarked_survival_counts, x='Embarked', y='Percentage', hue='Survived', palette='Set1')\n\n# Aggiungere titolo e etichette\naxes[1].set_title('Survival Percentage Based on Embarked Location')\naxes[1].set_xlabel('Port of Embarkation')\naxes[1].set_ylabel('Percentage of Passengers')\n\n# Regolazione del layout per evitare sovrapposizioni\nplt.tight_layout()\n\n# Mostrare il grafico\nplt.show()\n\n\n# Crea una griglia di 3 grafici su una riga\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Istogramma di Fare\naxes[0].hist(train_data['Fare'], bins=50, color='blue', edgecolor='black')\naxes[0].set_title('Distribuzione della variabile Fare')\naxes[0].set_xlabel('Fare')\naxes[0].set_ylabel('Frequenza')\n\n# Boxplot di Fare\nsns.boxplot(x=train_data['Fare'], color='lightblue', ax=axes[1])\naxes[1].set_title('Boxplot della variabile Fare')\naxes[1].set_xlabel('Fare')\n\n# Boxplot di Fare suddiviso per Survived\nsns.boxplot(x='Survived', y='Fare', data=train_data, ax=axes[2])\naxes[2].set_title('Distribuzione di Fare per Survived')\naxes[2].set_xlabel('Survived')\naxes[2].set_ylabel('Fare')\n\n# Regolazione del layout per evitare sovrapposizioni\nplt.tight_layout()\n\n# Visualizzazione dei grafici\nplt.show()\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Creiamo dei bin per la variabile Fare per semplificare la visualizzazione\ntrain_data['Fare_bin'] = pd.qcut(train_data['Fare'], 4)  # Divide Fare in quartili\n\n# Creiamo una tabella pivot per visualizzare la sopravvivenza in base a Pclass e Fare_bin\npivot_table = train_data.pivot_table(values='Survived', index='Pclass', columns='Fare_bin', aggfunc='mean', observed=False)\n\n# Visualizziamo la heatmap\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\nsns.heatmap(pivot_table, annot=True, cmap='YlGnBu', ax=axes[0])\naxes[0].set_title(\"Heatmap delle interazioni tra Pclass e Fare rispetto alla sopravvivenza\")\n\nsns.scatterplot(x='Pclass', y='Fare', hue='Survived', data=train_data, ax=axes[1])\naxes[1].set_title('Scatter plot Pclass and Fare for Survived label')\n\nplt.show()\n\ntest_data['Fare'] = imputer.fit_transform(test_data[['Fare']])\ntrain_data['Pclass_Fare'] =  train_data['Fare']/train_data['Pclass'] \ntest_data['Pclass_Fare'] =  test_data['Fare']/test_data['Pclass'] \n\n# Istogramma della variabile Fare*Pclass\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\naxes[0].hist(train_data['Pclass_Fare'], bins=30, color='skyblue', edgecolor='black')\naxes[0].set_title('Histogram of Pclass*Fare')\naxes[0].set_xlabel('Pclass/Fare')\naxes[0].set_ylabel('Frequency')\n\n# Boxplot della variabile Fare*Pclass\naxes[1].boxplot(train_data['Pclass_Fare'], vert=False)\naxes[1].set_title('Boxplot of Pclass/Fare')\naxes[1].set_xlabel('Pclass/Fare')\n\nplt.tight_layout()\nplt.show()\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Crea una nuova colonna 'TicketCategory' con la prima lettera del biglietto, o categorizzazioni per mancanza di lettera\ntrain_data['TicketCategory'] = train_data['Ticket'].apply(lambda x: str(x)[0] if pd.notnull(x) and str(x)[0].isalpha() else 'NoLetter' if pd.notnull(x) else 'Missing')\ntest_data['TicketCategory'] = test_data['Ticket'].apply(lambda x: str(x)[0] if pd.notnull(x) and str(x)[0].isalpha() else 'NoLetter' if pd.notnull(x) else 'Missing')\n# Calcola la probabilità di sopravvivenza per ciascuna categoria di biglietto\nticket_survival = train_data.groupby('TicketCategory')['Survived'].mean() * 100  # Percentuale di sopravvivenza\n\n# Ordina per comodità di visualizzazione\nticket_survival = ticket_survival.sort_values()\n\n# Crea il grafico\nfig, axes = plt.subplots(1, 2, figsize=(18, 6))\nsns.barplot(x=ticket_survival.index, y=ticket_survival.values, palette=\"viridis\", ax=axes[0])\naxes[0].set_xlabel(\"Ticket Category\")\naxes[0].set_ylabel(\"Survival Probability (%)\")\naxes[0].set_title(\"Survival Probability by Ticket Category\")\n\n# Crea una tabella di frequenza per le categorie di biglietto e la classe\nticket_class_counts = train_data.groupby(['TicketCategory', 'Pclass']).size().unstack(fill_value=0)\n\n# Crea il grafico\nticket_class_counts.plot(kind=\"bar\", stacked=True, colormap=\"viridis\", width=0.8, ax=axes[1])\naxes[1].set_xlabel(\"Ticket Category\")\naxes[1].set_ylabel(\"Number of Passengers\")\naxes[1].set_title(\"Ticket Category Distribution by Class\")\naxes[1].legend(title=\"Class\", labels=[\"1st\", \"2nd\", \"3rd\"])\nplt.xticks(rotation=45)\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:42:17.750799Z","iopub.execute_input":"2024-10-31T20:42:17.751223Z","iopub.status.idle":"2024-10-31T20:42:22.242760Z","shell.execute_reply.started":"2024-10-31T20:42:17.751186Z","shell.execute_reply":"2024-10-31T20:42:22.241722Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data['Age'] = imputer.fit_transform(train_data[['Age']])\ntest_data['Age'] = imputer.fit_transform(test_data[['Age']])\n\n\n# Conversione della colonna \"Sex\"\ntrain_data['Sex'] = train_data['Sex'].map({'male': 0, 'female': 1})\ntest_data['Sex'] = test_data['Sex'].map({'male': 0, 'female': 1})\n\n#Gestione titolo\n# Estrazione del titolo dal nome\ntrain_data['Title'] = train_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_data['Title'] = test_data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# Raggruppa titoli rari\ntrain_data['Title'] = train_data['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',\n                                                   'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntrain_data['Title'] = train_data['Title'].replace('Mlle', 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Ms', 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Mme', 'Mrs')\n\ntest_data['Title'] = test_data['Title'].replace(['Lady', 'Countess', 'Capt', 'Col',\n                                                   'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_data['Title'] = test_data['Title'].replace('Mlle', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Ms', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Mme', 'Mrs')\n\n# Converti i titoli in numeri\ntrain_data = pd.get_dummies(train_data, columns=['Title'], drop_first=True)\ntest_data = pd.get_dummies(test_data, columns=['Title'], drop_first=True)\n\n#Creazione FamilySize e IsAlone feature\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n\ntrain_data.loc[train_data['FamilySize'] > 1, 'IsAlone'] = 0\ntrain_data.loc[train_data['FamilySize'] == 1, 'IsAlone'] = 1\ntest_data.loc[train_data['FamilySize'] > 1, 'IsAlone'] = 0\ntest_data.loc[train_data['FamilySize'] == 1, 'IsAlone'] = 1\n\n#Creazione di AgeGroup\ntrain_data['AgeGroup'] = pd.cut(train_data['Age'], bins=[0.0, 1.0, 4.0, 11.0, 17.0, 24.0, 64.0, 100.0],\n                 labels=['Infants', 'Toddlers', 'Children', 'Adolescents', \n                         'Young Adults', 'Adults', 'Seniors'])\ntest_data['AgeGroup'] = pd.cut(test_data['Age'], bins=[0.0, 1.0, 4.0, 11.0, 17.0, 24.0, 64.0, 100.0],\n                 labels=['Infants', 'Toddlers', 'Children', 'Adolescents', \n                         'Young Adults', 'Adults', 'Seniors'])\n\n\n# Calcolo della percentuale di sopravvivenza per gruppo di età\nage_survival_df = train_data.groupby('AgeGroup', observed=False)['Survived'].mean().reset_index()\nage_survival_df['Survived'] = age_survival_df['Survived'] * 100  # Converti in percentuale\n\n# Grafico\nplt.figure(figsize=(10, 6))\nsns.barplot(data=age_survival_df, x='AgeGroup', y='Survived', palette=\"viridis\")\nplt.title(\"Probabilità di Sopravvivenza per Gruppo di Età\")\nplt.xlabel(\"Gruppo di Età\")\nplt.ylabel(\"Percentuale di Sopravvivenza\")\nplt.show()\n\n# Crea le variabili dummies per 'AgeGroup'\ntrain_data = pd.get_dummies(train_data, columns=['AgeGroup'], drop_first=True)\ntest_data = pd.get_dummies(test_data, columns=['AgeGroup'], drop_first=True)\n\n#Crea le variabili dummies per 'Embarked'\ntrain_data = pd.get_dummies(train_data, columns=['Embarked'], drop_first=True)\ntest_data = pd.get_dummies(test_data, columns=['Embarked'], drop_first=True)\n\n#Crea le variabili dummiese per 'TicketCategory'\ntrain_data = pd.get_dummies(train_data, columns=['TicketCategory'], drop_first=True)\ntest_data = pd.get_dummies(test_data, columns=['TicketCategory'], drop_first=True)\n\n'''\n#Dividere la variabile Fare in Bin e Creazione variabili Dummies\ntrain_data['FareBin'] = pd.cut(train_data['Fare'], bins=[0, 7.91, 14.454, 31, 512], \n                               labels=['Low', 'Medium', 'High', 'Very High'])\ntest_data['FareBin'] = pd.cut(test_data['Fare'], bins=[0, 7.91, 14.454, 31, 512], \n                               labels=['Low', 'Medium', 'High', 'Very High'])\ntrain_data = pd.get_dummies(train_data, columns=['FareBin'], drop_first=True)\ntest_data = pd.get_dummies(test_data, columns=['FareBin'], drop_first=True)\n\n#GetDummies per la variabile Pclass per aiutare alcuni modelli\ntrain_data = pd.get_dummies(train_data, columns=['Pclass'], drop_first=True)\ntest_data = pd.get_dummies(test_data, columns=['Pclass'], drop_first=True)\n'''\n\n# Verifica il risultato\nprint(train_data.head())\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nsns.boxplot(data=train_data, x='Pclass', y='Fare', hue='FamilySize', ax=axes[0])\naxes[0].set_title(\"Fare per Classe e Dimensione della Famiglia\")\n\n\n# Calcolare la matrice di correlazione\ncorr_matrix = train_data[['Fare', 'FamilySize', 'Pclass']].corr()\n\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[1])\naxes[1].set_title(\"Correlazione tra Fare, Classe e FamilySize\")\n\n\nsns.kdeplot(data=train_data, x='Fare', hue='Pclass', multiple='stack', ax=axes[2])\naxes[2].set_title(\"Distribuzione del Fare per Classe\")\n\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:42:29.401184Z","iopub.execute_input":"2024-10-31T20:42:29.401759Z","iopub.status.idle":"2024-10-31T20:42:30.946106Z","shell.execute_reply.started":"2024-10-31T20:42:29.401716Z","shell.execute_reply":"2024-10-31T20:42:30.945039Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_data=train_data.drop(['Name','Cabin','Ticket', 'SibSp','Parch','Age','Fare_bin','Pclass', 'Fare'],axis=1)\ntest_data=test_data.drop(['Name','Cabin','Ticket', 'SibSp','Parch','Age','Pclass','Fare'],axis=1)\nX_val = test_data.drop(columns = ['PassengerId'])\nX = train_data.drop(columns=['Survived','PassengerId'])\ny = train_data['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:43:43.301002Z","iopub.execute_input":"2024-10-31T20:43:43.301959Z","iopub.status.idle":"2024-10-31T20:43:43.317373Z","shell.execute_reply.started":"2024-10-31T20:43:43.301918Z","shell.execute_reply":"2024-10-31T20:43:43.316185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport numpy as np\nX_train_dta = X_train\nX_test_dta = X_test\nX_val_dta = X_val\nX_train_dta.head()\n\n\nclf1 = LogisticRegression(penalty = 'l2', C = 0.1, solver = 'saga', random_state = 1, class_weight = 'balanced')\nclf2 = DecisionTreeClassifier(max_depth = 5, criterion = 'entropy', random_state = 0, class_weight = 'balanced')\nclf3 = KNeighborsClassifier(n_neighbors = 7, p = 2, metric = 'minkowski')\nclf4 = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nclf5 = xgb.XGBClassifier(n_estimators = 1500, learning_rate =0.08, max_depth=5, eval_metric='logloss', random_state=42)\npipe1 = Pipeline([['sc', StandardScaler()], ['clf1', clf1]])\npipe2 = Pipeline([['clf2', clf2]])\npipe3 = Pipeline([['sc', StandardScaler()],['clf3', clf3]])\npipe4 = Pipeline([['sc', StandardScaler()], ['clf4', clf4]])\npipe5 = Pipeline([['sc', StandardScaler()],['clf5', clf5]])\n#'Logistic regression', 'Decision Tree',\nclf_labels = ['Logistic regression', 'Decision Tree', 'KNN', 'Random Forest', 'XGBoost']\nprint('10-fold cross validation:\\n')\npipes = [pipe1, pipe2, pipe3,pipe4, pipe5]\n\n# Festure importance per RandomForestClassifier e XGboost\n\n# 1. Addestramento del modello\nX_train_dta_np = X_train_dta.values  # .values restituisce l'array senza i nomi delle feature\n\npipe1.fit(X_train_dta, y_train)  # Logistic Regression\npipe2.fit(X_train_dta_np, y_train)  # Decision Tree\npipe3.fit(X_train_dta, y_train)  # KNN\npipe4.fit(X_train_dta, y_train)  # Random Forest\npipe5.fit(X_train_dta, y_train)  # XGBoost\n\n#clf4.fit(X_train, y_train)\n\n# 2. Estrazione delle importanze\nimportances = clf4.feature_importances_\n\n# 3. Visualizzazione delle importanze\n# Creiamo un dataframe per le importanze\nfeat_importances = pd.Series(importances, index=X_train_dta.columns)\nfeat_importances = feat_importances.sort_values(ascending=False)\n\n# Grafico\nplt.figure(figsize=(10, 6))\nfeat_importances.plot(kind='bar')\nplt.title('Feature Importance - Random Forest')\nplt.show()\n\n# 1. Addestramento del modello\nclf5.fit(X_train_dta, y_train)\n\n# 2. Visualizzazione delle importanze (usando plot_importance)\nplt.figure(figsize=(10, 6))\nxgb.plot_importance(clf5, max_num_features=15)  # Mostra le 10 feature più importanti\nplt.title('Feature Importance - XGBoost')\nplt.show()\n\nfrom sklearn.inspection import permutation_importance\nimport matplotlib.pyplot as plt\n\n# Lista di classificatori e nomi\n\nclassifiers = {\n    'Logistic Regression': pipe1.named_steps['clf1'],\n    'Decision Tree': pipe2.named_steps['clf2'],\n    'KNN': pipe3.named_steps['clf3'],\n    'Random Forest': pipe4.named_steps['clf4'],\n    'XGBoost': pipe5.named_steps['clf5']\n}\n\n# Per ogni classificatore, calcola l'importanza delle feature e visualizza il grafico\n# Dizionario per salvare le importanze delle feature per ciascun modello\nfeature_importances = pd.DataFrame()\n\n# Calcolo delle importanze delle feature per ciascun modello\nfor name, clf in classifiers.items():\n    results = permutation_importance(clf, X_train_dta_np, y_train, n_repeats=10, random_state=42, n_jobs=-1)\n    feat_importances = pd.Series(results.importances_mean, index=X_train_dta.columns)\n    \n    # Aggiungiamo i risultati al DataFrame\n    feature_importances[name] = feat_importances\n\n# Trasformiamo il DataFrame per avere un formato lungo (necessario per il grafico)\nfeature_importances = feature_importances.reset_index().melt(id_vars=\"index\", \n                                                             var_name=\"Modello\", \n                                                             value_name=\"Importanza\")\nfeature_importances = feature_importances.rename(columns={\"index\": \"Feature\"})\n\n# Plotting\nplt.figure(figsize=(12, 8))\nsns.barplot(data=feature_importances, x=\"Feature\", y=\"Importanza\", hue=\"Modello\")\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Importanza delle feature per ciascun modello\")\nplt.ylabel(\"Mean decrease in accuracy\")\nplt.legend(title=\"Modello\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:44:54.816182Z","iopub.execute_input":"2024-10-31T20:44:54.816681Z","iopub.status.idle":"2024-10-31T20:45:11.167455Z","shell.execute_reply.started":"2024-10-31T20:44:54.816626Z","shell.execute_reply":"2024-10-31T20:45:11.166061Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfeatures_lr = ['Title_Mr','Pclass_Fare', 'Sex', 'AgeGroup_Adults','AgeGroup_Young Adults','AgeGroup_Adolescents','Embarked_S','Embarked_Q']\nfeatures_dt = ['Title_Mr','Pclass_Fare','FamilySize','Title_Rare','AgeGroup_Young Adults','TicketCategory_S', 'TicketCategory_W']\nfeatures_knn = ['Pclass_Fare','Title_Mr','Sex']\nfeatures_rf = ['Sex','FamilySize','Title_Mrs','Title_Miss']\nfeatures_xg = ['Pclass_Fare','Title_Mr','FamilySize','AgeGroup_Adults','Embarked_S','TicketCategory_NoLetter','TicketCategory_S','AgeGroup_Young Adults','AgeGroup_Children','Embarked_Q']\n\nX_train_dta_LR = X_train_dta[features_lr]\nX_train_dta_DT = X_train_dta[features_dt]\nX_train_dta_KNN = X_train_dta[features_knn]\nX_train_dta_RF = X_train_dta[features_rf]\nX_train_dta_XG = X_train_dta[features_xg]\n\nX_test_dta_LR = X_test_dta[features_lr]\nX_test_dta_DT = X_test_dta[features_dt]\nX_test_dta_KNN = X_test_dta[features_knn]\nX_test_dta_RF = X_test_dta[features_rf]\nX_test_dta_XG = X_test_dta[features_xg]\n\nX_val_dta_LR = X_val_dta[features_lr]\nX_val_dta_DT = X_val_dta[features_dt]\nX_val_dta_KNN = X_val_dta[features_knn]\nX_val_dta_RF = X_val_dta[features_rf]\nX_val_dta_XG = X_val_dta[features_xg]\n\n#Valutazione Accuracy sui singoli modelli con le features specifiche\nprint('10-fold cross validation:\\n')\nscores_LR = cross_val_score(estimator=pipe1, X = X_train_dta_LR, y=y_train, cv=10, scoring='accuracy')\nprint(f'Accuracy: {scores_LR.mean():.2f}'\n      f'(+/- {scores_LR.std():-2f}) Logistic Regression')\n\nscores_DT = cross_val_score(estimator=pipe2, X = X_train_dta_DT, y=y_train, cv=10, scoring='accuracy')\nprint(f'Accuracy: {scores_DT.mean():.2f}'\n      f'(+/- {scores_DT.std():-2f}) Decision Tree')\n\nscores_KNN = cross_val_score(estimator=pipe3, X = X_train_dta_KNN, y=y_train, cv=10, scoring='accuracy')\nprint(f'Accuracy: {scores_KNN.mean():.2f}'\n      f'(+/- {scores_KNN.std():-2f}) KNN')\n\nscores_RF = cross_val_score(estimator=pipe4, X = X_train_dta_RF, y=y_train, cv=10, scoring='accuracy')\nprint(f'Accuracy: {scores_RF.mean():.2f}'\n      f'(+/- {scores_RF.std():-2f}) Random Forest')\n\nscores_XG = cross_val_score(estimator=pipe5, X = X_train_dta_XG, y=y_train, cv=10, scoring='accuracy')\nprint(f'Accuracy: {scores_XG.mean():.2f}'\n      f'(+/- {scores_XG.std():-2f}) XGBoost')\n\n#Allenamento modelli, fit e valutazione accuracy\npipe1.fit(X_train_dta_LR, y_train)  # Logistic Regression\npipe2.fit(X_train_dta_DT, y_train)  # Decision Tree\npipe3.fit(X_train_dta_KNN, y_train)  # KNN\npipe4.fit(X_train_dta_RF, y_train)  # Random Forest\npipe5.fit(X_train_dta_XG, y_train)  # XGBoost\n\ny_train_dta_LR = pipe1.predict(X_train_dta_LR)\ny_train_dta_DT = pipe2.predict(X_train_dta_DT)\ny_train_dta_KNN = pipe3.predict(X_train_dta_KNN)\ny_train_dta_RF = pipe4.predict(X_train_dta_RF)\ny_train_dta_XG = pipe5.predict(X_train_dta_XG)\n\ny_test_dta_LR = pipe1.predict(X_test_dta_LR)\ny_test_dta_DT = pipe2.predict(X_test_dta_DT)\ny_test_dta_KNN = pipe3.predict(X_test_dta_KNN)\ny_test_dta_RF = pipe4.predict(X_test_dta_RF)\ny_test_dta_XG = pipe5.predict(X_test_dta_XG)\n\ny_val_dta_LR = pipe1.predict(X_val_dta_LR)\ny_val_dta_DT = pipe2.predict(X_val_dta_DT)\ny_val_dta_KNN = pipe3.predict(X_val_dta_KNN)\ny_val_dta_RF = pipe4.predict(X_val_dta_RF)\ny_val_dta_XG = pipe5.predict(X_val_dta_XG)\n\n#Assemblaggio delle predizione e valutazione delle accuracy\ny_pred_dta_train = np.round((scores_LR.mean()*y_train_dta_LR + scores_DT.mean()*y_train_dta_DT + scores_KNN.mean()*y_train_dta_KNN + \n                                scores_RF.mean()*y_train_dta_RF + scores_XG.mean()*y_train_dta_XG) / (scores_LR.mean()+scores_DT.mean()+scores_KNN.mean()+scores_RF.mean()+scores_XG.mean())).astype(int)\ny_pred_dta_test = np.round((scores_LR.mean()*y_test_dta_LR + scores_DT.mean()*y_test_dta_DT + scores_KNN.mean()*y_test_dta_KNN + \n                                scores_RF.mean()*y_test_dta_RF + scores_XG.mean()*y_test_dta_XG) / (scores_LR.mean()+scores_DT.mean()+scores_KNN.mean()+scores_RF.mean()+scores_XG.mean())).astype(int)\n\ny_pred_dta_val = np.round((scores_LR.mean()*scores_LR.mean()*y_val_dta_LR + scores_DT.mean()*y_val_dta_DT + scores_KNN.mean()*y_val_dta_KNN + \n                                scores_RF.mean()*y_val_dta_RF + scores_XG.mean()*y_val_dta_XG) / (scores_LR.mean()+scores_DT.mean()+scores_KNN.mean()+scores_RF.mean()+scores_XG.mean())).astype(int)\n\nfp_dta_train_acc = accuracy_score(y_train, y_pred_dta_train)\nfp_dta_test_acc = accuracy_score(y_test, y_pred_dta_test)\n\nprint(f'Accuracy_train/accuracy_test '\n      f'{fp_dta_train_acc:.3f}/{fp_dta_test_acc:.3f}')\n\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# 1. Addestrare i modelli di base e ottenere le previsioni su X_train e X_val\ntrain_meta_features = [y_train_dta_LR, y_train_dta_DT, y_train_dta_KNN, y_train_dta_RF, y_train_dta_XG]\nval_meta_features = [y_val_dta_LR, y_val_dta_DT, y_val_dta_KNN, y_val_dta_RF, y_val_dta_XG]\ntest_meta_features = [y_test_dta_LR, y_test_dta_DT, y_test_dta_KNN, y_test_dta_RF, y_test_dta_XG]\n\n\n# Trasforma le liste in matrici per il meta-modello\ntrain_meta_features = np.array(train_meta_features).T  # Trasposta per ottenere una riga per ciascun esempio\nval_meta_features = np.array(val_meta_features).T\ntest_meta_features = np.array(test_meta_features).T\n\n# 2. Addestrare il meta-modello Bernoulli Naive Bayes\nmeta_model = BernoulliNB()\nmeta_model.fit(train_meta_features, y_train)\n\n# 3. Valutare il meta-modello\ntrain_preds = meta_model.predict(train_meta_features)\nval_preds = meta_model.predict(val_meta_features)\ntest_preds = meta_model.predict(test_meta_features)\n\n# Calcolare l'accuracy\ntrain_accuracy = accuracy_score(y_train, train_preds)\ntest_accuracy = accuracy_score(y_test, test_preds)\n\nprint(f\"Train Accuracy del meta-modello: {train_accuracy:.4f}\")\nprint(f\"Test Accuracy del meta-modello: {test_accuracy:.4f}\")\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': val_preds})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T20:56:36.343721Z","iopub.execute_input":"2024-10-31T20:56:36.344119Z","iopub.status.idle":"2024-10-31T20:56:57.096591Z","shell.execute_reply.started":"2024-10-31T20:56:36.344082Z","shell.execute_reply":"2024-10-31T20:56:57.095508Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Assumiamo che le feature siano già suddivise per ciascun modello\n# Dividi X_train, X_test, X_val nelle feature specifiche per ciascun modello\n\n# Sottinsiemi di feature per ciascun modello (esempio)\nX_train_lr, X_test_lr, X_val_lr = X_train[features_lr], X_test[features_lr], X_val[features_lr]\nX_train_dt, X_test_dt, X_val_dt = X_train[features_dt], X_test[features_dt], X_val[features_dt]\nX_train_knn, X_test_knn, X_val_knn = X_train[features_knn], X_test[features_knn], X_val[features_knn]\nX_train_rf, X_test_rf, X_val_rf = X_train[features_rf], X_test[features_rf], X_val[features_rf]\nX_train_xg, X_test_xg, X_val_xg = X_train[features_xg], X_test[features_xg], X_val[features_xg]\n\n# Creazione dei modelli base\nlr = LogisticRegression(max_iter=1000, random_state=42)\ndt = DecisionTreeClassifier(random_state=42)\nknn = KNeighborsClassifier(n_neighbors=5)\n#rf = RandomForestClassifier(n_estimators=100, random_state=42)\n#xg = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n\n# Dizionario con i modelli e i rispettivi subset di feature\nbase_classifiers = [\n    ('lr', lr, X_train_lr, X_test_lr, X_val_lr),\n   # ('dt', dt, X_train_dt, X_test_dt, X_val_dt),\n    ('knn', knn, X_train_knn, X_test_knn, X_val_knn),\n   # ('rf', rf, X_train_rf, X_test_rf, X_val_rf),\n   # ('xg', xg, X_train_xg, X_test_xg, X_val_xg)\n]\n\n# Addestramento dei modelli base sui rispettivi subset di feature\nfor name, clf, X_tr, X_te, X_va in base_classifiers:\n    print(f\"Addestramento del modello {name}...\")\n    clf.fit(X_tr, y_train)\n\n# Predizione con i modelli base per ottenere le probabilità come input al meta-modello\ntrain_meta_features = []\nval_meta_features = []\ntest_meta_features = []\n\nfor name, clf, X_tr, X_te, X_va in base_classifiers:\n    train_meta_features.append(clf.predict_proba(X_tr)[:, 1].reshape(-1, 1))\n    val_meta_features.append(clf.predict_proba(X_va)[:, 1].reshape(-1, 1))\n    test_meta_features.append(clf.predict_proba(X_te)[:, 1].reshape(-1, 1))\n\n# Stack delle feature per il meta-modello\ntrain_meta = np.hstack(train_meta_features)\nval_meta = np.hstack(val_meta_features)\ntest_meta = np.hstack(test_meta_features)\n\n# Definizione e addestramento del meta-modello (Boosting con XGBoost)\nmeta_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\nmeta_model.fit(train_meta, y_train)\n\n# Valutazione delle performance\ntrain_predictions = meta_model.predict(train_meta)\nval_predictions = meta_model.predict(val_meta)\ntest_predictions = meta_model.predict(test_meta)\n\nprint(\"Accuracy su Training set:\", accuracy_score(y_train, train_predictions))\n#print(\"Accuracy su Validation set:\", accuracy_score(y_val, val_predictions))\nprint(\"Accuracy su Test set:\", accuracy_score(y_test, test_predictions))\n","metadata":{}},{"cell_type":"markdown","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# Supponiamo di avere già diviso il dataset in train, validation e test\n# train_data è il dataset completo\n\n\n# Divisione dei dati in train, validation e test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.3, random_state=42)\n\n# Creazione dei modelli base con Bagging\nbagging_lr = BaggingClassifier(estimator=pipe1, n_estimators=10, random_state=42)\nbagging_dt = BaggingClassifier(estimator=pipe2, n_estimators=10, random_state=42)\nbagging_knn = BaggingClassifier(estimator=pipe3, n_estimators=10, random_state=42)\nbagging_rf = BaggingClassifier(estimator=pipe4, n_estimators=10, random_state=42)\nbagging_xg = BaggingClassifier(estimator=pipe5, n_estimators=10, random_state=42)\n\n# Addestramento dei modelli\nbagging_lr.fit(X_train[features_lr], y_train)\nbagging_dt.fit(X_train[features_dt], y_train)\nbagging_knn.fit(X_train[features_knn], y_train)\nbagging_rf.fit(X_train[features_rf], y_train)\nbagging_xg.fit(X_train[features_xg], y_train)\n\n# Predizioni sui set di validation e test\ntrain_preds_lr = bagging_lr.predict(X_train[features_lr])\ntrain_preds_dt = bagging_dt.predict(X_train[features_dt])\ntrain_preds_knn = bagging_knn.predict(X_train[features_knn])\ntrain_preds_rf = bagging_rf.predict(X_train[features_rf])\ntrain_preds_xg = bagging_xg.predict(X_train[features_xg])\n\ntest_preds_lr = bagging_lr.predict(X_test[features_lr])\ntest_preds_dt = bagging_dt.predict(X_test[features_dt])\ntest_preds_knn = bagging_knn.predict(X_test[features_knn])\ntest_preds_rf = bagging_rf.predict(X_test[features_rf])\ntest_preds_xg = bagging_xg.predict(X_test[features_xg])\n\nval_preds_lr = bagging_lr.predict(X_val_dta_LR)\nval_preds_dt = bagging_dt.predict(X_val_dta_DT)\nval_preds_knn = bagging_knn.predict(X_val_dta_KNN)\nval_preds_rf = bagging_rf.predict(X_val_dta_RF)\nval_preds_xg = bagging_xg.predict(X_val_dta_XG)\n\n# Combina le predizioni per majority voting (modello di ensemble finale)\nimport numpy as np\n\ntrain_preds = np.array([train_preds_lr, train_preds_dt, train_preds_knn, train_preds_rf, train_preds_xg])\ntest_preds = np.array([test_preds_lr, test_preds_dt, test_preds_knn, test_preds_rf, test_preds_xg])\nval_preds = np.array([val_preds_lr, val_preds_dt, val_preds_knn, val_preds_rf, val_preds_xg])\n\n# Majority voting\ntrain_final_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=train_preds)\ntest_final_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=test_preds)\nval_final_preds = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=val_preds)\n\n# Valutazione dell'accuracy finale\ntrain_accuracy = accuracy_score(y_train, train_final_preds)\ntest_accuracy = accuracy_score(y_test, test_final_preds)\n\nprint(f\"Train Accuracy: {train_accuracy:.4f}\")\nprint(f\"Test Accuracy: {test_accuracy:.4f}\")\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': y_pred_dta_val})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\n\n'''\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# Impostiamo il classificatore base e il Bagging model\nestimator = DecisionTreeClassifier()\nbagging_model = BaggingClassifier(estimator=estimator, random_state=42)\n\n# Definiamo la griglia di parametri\nparam_grid = {\n    'n_estimators': [10, 50, 100, 200],\n    'estimator__max_depth': [3, 5, 10, None],\n    'max_samples': [0.5, 0.7, 1.0],\n    'max_features': [0.5, 0.7, 1.0]\n}\n\n# Usare GridSearchCV per trovare la combinazione migliore\ngrid_search = GridSearchCV(bagging_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_search.fit(X_train, y_train)\n\n# Stampa i migliori parametri trovati e l'accuracy corrispondente\nprint(\"Best Parameters:\", grid_search.best_params_)\nprint(\"Best Accuracy:\", grid_search.best_score_)\n\n# Valutazione su set di test e validazione\nbest_bagging_model = grid_search.best_estimator_\ntrain_accuracyGV = best_bagging_model.score(X_train, y_train)\ntest_accuracyGV = best_bagging_model.score(X_test, y_test)\n\n\nprint(f\"Validation Accuracy GV: {train_accuracyGV:.4f}\")\nprint(f\"Test Accuracy GV: {test_accuracyGV:.4f}\")\n'''","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#Loss Curve for Logistic Regression\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\n\n# Definisci l'intervallo di valori di C da testare\nC_values = np.logspace(-4, 4, 20)  # da 10^-4 a 10^4\nlosses = []\n\n# Calcola la perdita media per ogni valore di C\nfor C in C_values:\n    clf = LogisticRegression(C=C, max_iter=1000)\n    # Cross-entropy loss (negative log-likelihood)\n    loss = -cross_val_score(clf, X_train_dta_LR, y_train, cv=5, scoring='neg_log_loss').mean()\n    losses.append(loss)\n\n# Plot della loss curve\nplt.figure(figsize=(10, 6))\nplt.plot(C_values, losses, marker='o')\nplt.xscale('log')\nplt.xlabel('C (Regularization parameter)')\nplt.ylabel('Log Loss')\nplt.title('Loss Curve for Logistic Regression')\nplt.show()","metadata":{}},{"cell_type":"markdown","source":"'''\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import Pipeline\nimport xgboost as xgb\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport numpy as np\n\n#Best parameters: {'pipe3__clf__n_neighbors': 7, \n#'pipe5__clf__learning_rate': 0.06, 'pipe5__clf__n_estimators': 2000}\n\nclf1 = LogisticRegression(penalty = 'l2', C = 0.001, solver = 'lbfgs', random_state = 1, class_weight = 'balanced')\nclf2 = DecisionTreeClassifier(max_depth = 1, criterion = 'entropy', random_state = 0, class_weight = 'balanced')\nclf3 = KNeighborsClassifier(n_neighbors = 7, p = 2, metric = 'minkowski')\nclf4 = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=1)\nclf5 = xgb.XGBClassifier(n_estimators = 1500, learning_rate =0.06, max_depth=5, random_state=42)\npipe1 = Pipeline([['sc', StandardScaler()], ['clf1', clf1]])\npipe2 = Pipeline([['clf2', clf2]])\npipe3 = Pipeline([['sc', StandardScaler()],['clf3', clf3]])\npipe4 = Pipeline([['sc', StandardScaler()], ['clf4', clf4]])\npipe5 = Pipeline([['sc', StandardScaler()],['clf5', clf5]])\n#'Logistic regression', 'Decision Tree',\nclf_labels = ['Logistic regression', 'Decision Tree', 'KNN', 'Random Forest', 'XGBoost']\nprint('10-fold cross validation:\\n')\npipes = [pipe1, pipe2, pipe3,pipe4, pipe5]\nfor clf, label in zip(pipes, clf_labels):\n    scores = cross_val_score(estimator=clf, X = X_train, y=y_train, cv=10, scoring='accuracy')\n    print(f'Accuracy: {scores.mean():.2f}'\n         f'(+/- {scores.std():-2f})[{label}]')\n\n\n# Festure importance per RandomForestClassifier e XGboost\n\n# 1. Addestramento del modello\nX_train_np = X_train.values  # .values restituisce l'array senza i nomi delle feature\n\npipe1.fit(X_train, y_train)  # Logistic Regression\npipe2.fit(X_train_np, y_train)  # Decision Tree\npipe3.fit(X_train, y_train)  # KNN\npipe4.fit(X_train, y_train)  # Random Forest\npipe5.fit(X_train, y_train)  # XGBoost\n\n#clf4.fit(X_train, y_train)\n\n# 2. Estrazione delle importanze\nimportances = clf4.feature_importances_\n\n# 3. Visualizzazione delle importanze\n# Creiamo un dataframe per le importanze\nfeat_importances = pd.Series(importances, index=X_train.columns)\nfeat_importances = feat_importances.sort_values(ascending=False)\n\n# Grafico\nplt.figure(figsize=(10, 6))\nfeat_importances.plot(kind='bar')\nplt.title('Feature Importance - Random Forest')\nplt.show()\n\n# 1. Addestramento del modello\nclf5.fit(X_train, y_train)\n\n# 2. Visualizzazione delle importanze (usando plot_importance)\nplt.figure(figsize=(10, 6))\nxgb.plot_importance(clf5, max_num_features=10)  # Mostra le 10 feature più importanti\nplt.title('Feature Importance - XGBoost')\nplt.show()\n\nfrom sklearn.inspection import permutation_importance\nimport matplotlib.pyplot as plt\n\n# Lista di classificatori e nomi\n\nclassifiers = {\n    'Logistic Regression': pipe1.named_steps['clf1'],\n    'Decision Tree': pipe2.named_steps['clf2'],\n    'KNN': pipe3.named_steps['clf3'],\n    'Random Forest': pipe4.named_steps['clf4'],\n    'XGBoost': pipe5.named_steps['clf5']\n}\n\n# Per ogni classificatore, calcola l'importanza delle feature e visualizza il grafico\n# Dizionario per salvare le importanze delle feature per ciascun modello\nfeature_importances = pd.DataFrame()\n\n# Calcolo delle importanze delle feature per ciascun modello\nfor name, clf in classifiers.items():\n    results = permutation_importance(clf, X_train_np, y_train, n_repeats=10, random_state=42, n_jobs=-1)\n    feat_importances = pd.Series(results.importances_mean, index=X_train.columns)\n    \n    # Aggiungiamo i risultati al DataFrame\n    feature_importances[name] = feat_importances\n\n# Trasformiamo il DataFrame per avere un formato lungo (necessario per il grafico)\nfeature_importances = feature_importances.reset_index().melt(id_vars=\"index\", \n                                                             var_name=\"Modello\", \n                                                             value_name=\"Importanza\")\nfeature_importances = feature_importances.rename(columns={\"index\": \"Feature\"})\n\n# Plotting\nplt.figure(figsize=(12, 8))\nsns.barplot(data=feature_importances, x=\"Feature\", y=\"Importanza\", hue=\"Modello\")\nplt.xticks(rotation=45, ha='right')\nplt.title(\"Importanza delle feature per ciascun modello\")\nplt.ylabel(\"Mean decrease in accuracy\")\nplt.legend(title=\"Modello\")\nplt.tight_layout()\nplt.show()\n'''","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"'''\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\n\n#'pipe1__clf__C': [0.001, 0.01, 0.1, 1],\n#    'pipe2__clf__max_depth': [1, 2, 3, 5],\n# Definisci il dizionario dei parametri per il tuning\nparam_grid = {\n    'pipe1__clf1__C': [0.01, 0.1],\n    'pipe2__clf2__max_depth': [5, 8],\n    'pipe3__clf3__n_neighbors': [7, 3],\n    'pipe4__clf4__n_estimators': [100, 200],\n    'pipe4__clf4__max_depth': [5, 7],\n    'pipe5__clf5__learning_rate': [0.08, 0.01],\n    'pipe5__clf5__n_estimators': [1500, 100]\n}\n\n# Crea il VotingClassifier con le pipeline\n#('pipe1', pipe1), ('pipe2', pipe2), ('pipe4', pipe4),\nvoting_clf = VotingClassifier(estimators=[\n    ('pipe1', pipe1), ('pipe2', pipe2),\n    ('pipe3', pipe3), \n    ('pipe4', pipe4),\n    ('pipe5', pipe5)], voting='hard')\n\n# Inizia la ricerca grid search\ngrid = GridSearchCV(estimator=voting_clf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\ngrid.fit(X_train_dta, y_train)\n\nprint(f\"Best parameters: {grid.best_params_}\")\nprint(f\"Best cross-validation accuracy: {grid.best_score_}\")\n'''","metadata":{}},{"cell_type":"markdown","source":"Best parameters: {'pipe1__clf1__C': 0.01, 'pipe2__clf2__max_depth': 5, 'pipe3__clf3__n_neighbors': 7, 'pipe4__clf4__max_depth': 5, 'pipe4__clf4__n_estimators': 100, 'pipe5__clf5__learning_rate': 0.08, 'pipe5__clf5__n_estimators': 1500}\nBest cross-validation accuracy: 0.830089628681178","metadata":{}},{"cell_type":"markdown","source":"'''\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creazione del VotingClassifier con i classificatori\n#('lr', pipe1), \n#    ('DTC', pipe2),\nvoting_clf = VotingClassifier(estimators=[ ('lr', pipe1),('DTC', pipe2),   \n    ('KNN', pipe3), \n    ('RF', pipe4), \n    ('XG', pipe5)], voting='hard')\n\n# Addestramento\nvoting_clf.fit(X_train_dta, y_train)\n\n\nfrom sklearn.inspection import PartialDependenceDisplay\n\n# Modifica la chiamata\npipe4.fit(X_train, y_train)\nPartialDependenceDisplay.from_estimator(pipe4, X_train, [('Pclass', 'Fare')])\n\nplt.show()\n\npipe5.fit(X_train, y_train)\nPartialDependenceDisplay.from_estimator(pipe5, X_train, [('Pclass', 'Fare')])\n\nplt.show()\n\n# Previsioni\ny_train_pred = voting_clf.predict(X_train_dta)\ny_test_pred = voting_clf.predict(X_test_dta)\nMajoring_vote_train = accuracy_score(y_train, y_train_pred)\nMajoring_vote_test = accuracy_score(y_test, y_test_pred)\nprint(f'MV train/test accuracies '\n      f'{Majoring_vote_train:.3f}/{Majoring_vote_test:.3f}')\n\npredictions = voting_clf.predict(X_val_dta)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n'''","metadata":{}},{"cell_type":"markdown","source":"'''\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Creazione del VotingClassifier con i classificatori\n#('lr', pipe1), \n#    ('DTC', pipe2),\nvoting_clf = VotingClassifier(estimators=[ ('lr', pipe1),('DTC', pipe2),   \n    ('KNN', pipe3), \n    ('RF', pipe4), \n    ('XG', pipe5)], voting='hard')\n\n# Addestramento\nvoting_clf.fit(X_train_dta, y_train)\n\n\n'''\n#Stardadize the features\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.metrics import accuracy_score\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\nX_val_std = sc.transform(X_val)\n\nlda = LDA(n_components=1)\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1, class_weight='balanced')\nX_train_lda = lda.fit_transform(X_train_std, y_train)\nX_test_lda = lda.transform(X_test_std)\nX_val_lda = lda.transform(X_val_std)\nmodel.fit(X_train_lda, y_train)\ny_train_pred = model.predict(X_train_lda)\ny_test_pred = model.predict(X_test_lda)\nForest_train = accuracy_score(y_train, y_train_pred)\nForest_test = accuracy_score(y_test, y_test_pred)\nprint(f'Forest train/test accuracies '\n      f'{Forest_train:.3f}/{Forest_test:.3f}')\n\npredictions = model.predict(X_val_lda)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n'''","metadata":{}},{"cell_type":"markdown","source":"'''\nfrom sklearn.linear_model import LogisticRegression\n​\n# Selezionare le feature per il modello della prima e seconda classe\nX_train_first_second_class = first_second_class[[\"Sex\", \"SibSp\", \"Parch\", \"Fare\"]]  # Usa le tue feature\nX_train_first_second_class = pd.get_dummies(X_train_first_second_class)\ny_train_first_second_class = first_second_class[\"Survived\"]\n​\n# Creare e addestrare il modello per la prima e seconda classe\nmodel_first_second_class = LogisticRegression()\nmodel_first_second_class.fit(X_train_first_second_class, y_train_first_second_class)\ny_train_pred_first_second_class = model_first_second_class.predict(X_train_first_second_class)\n​\n# Selezionare le feature per il modello della terza classe\nX_train_third_class = third_class[[\"Sex\", \"SibSp\", \"Parch\", \"Fare\"]]  # Usa le tue feature\nX_train_third_class = pd.get_dummies(X_train_third_class)\ny_train_third_class = third_class[\"Survived\"]\n​\n# Creare e addestrare il modello per la prima e seconda classe\nmodel_third_class = LogisticRegression()\nmodel_third_class.fit(X_train_third_class, y_train_third_class)\ny_train_pred_third_class = model_third_class.predict(X_train_third_class)\n​\n​\n​\n# Feature per ciascuna classe\nX_test_first_second_class = pd.get_dummies(first_second_class_test[[\"Sex\", \"SibSp\", \"Parch\", \"Fare\"]])\nX_test_third_class = pd.get_dummies(third_class_test[[\"Sex\", \"SibSp\", \"Parch\", \"Fare\"]])\n​\n# Predizioni per ciascuna classe\ny_pred_first_second_class = model_first_second_class.predict(X_test_first_second_class)\ny_pred_third_class = model_third_class.predict(X_test_third_class)\n​\nimport numpy as np\n​\n# Combinare le predizioni per tutte le classi\ny_pred_combined = np.concatenate([y_train_pred_first_second_class, y_train_pred_third_class])\ny_train = np.concatenate([y_train_first_second_class, y_train_third_class]) \n​\nfrom sklearn.metrics import accuracy_score\n​\n# Calcolare l'accuratezza globale\nglobal_accuracy = accuracy_score(y_train, y_pred_combined)\nprint(f'Accuratezza Globale: {global_accuracy:.3f}')\n​\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n'''","metadata":{}},{"cell_type":"code","source":"'''\n#Selection Features through PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Stardadize the features\nsc = StandardScaler()\nX_train_std = sc.fit_transform(X_train)\nX_test_std = sc.transform(X_test)\nX_val_std = sc.transform(X_val)\n\ncov_mat = np.cov(X_train_std.T)\neigen_vals, eigen_vecs = np.linalg.eig(cov_mat)\nprint('\\nEigenvalues \\n', eigen_vals)\n\ntot = sum(eigen_vals)\nvar_exp =[(i/tot) for i in sorted(eigen_vals, reverse = True)]\ncum_var_exp = np.cumsum(var_exp)\n\nplt.bar(range(1,11), var_exp, align = 'center', label = 'Individual explained variance')\nplt.step(range(1,11), cum_var_exp, where = 'mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Pricipal component index')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:57:06.659907Z","iopub.status.idle":"2024-10-27T21:57:06.660445Z","shell.execute_reply.started":"2024-10-27T21:57:06.660196Z","shell.execute_reply":"2024-10-27T21:57:06.660235Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n#Feature Transformation\n#Make a list of (eigenvalue, eigenvector) tuples\neigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:,i]) for i in range(len(eigen_vals))]\neigen_pairs.sort(key = lambda k: k[0], reverse = True)\n#Selection of the first two eigenvectors\nw = np.hstack((eigen_pairs[0][1][:,np.newaxis],\n              eigen_pairs[1][1][:,np.newaxis]))\nprint('Matrix W:\\n', w)\nX_train_pca = X_train_std.dot(w)\n\ncolors = ['r','b']\nmarkers = ['o','s']\nfor l, c, m in zip(np.unique(y_train), colors, markers):\n    plt.scatter(X_train_pca[y_train==l, 0],\n               X_train_pca[y_train==l, 1],\n               c=c, label=f'Class {l}', marker = m)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.legend(loc = 'lower left')\nplt.tight_layout()\nplt.show()\n'''\n'''\nfrom sklearn.ensemble import RandomForestClassifier\n#from sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\n\npca = PCA(n_components=2)\n#lr = LogisticRegression(multi_class='ovr', random_state=1, solver='lbfgs')\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nX_train_pca = pca.fit_transform(X_train_std)\nX_test_pca = pca.transform(X_test_std)\nX_val_pca = pca.transform(X_val_std)\nmodel.fit(X_train_pca, y_train)\ny_train_pred = model.predict(X_train_pca)\ny_test_pred = model.predict(X_test_pca)\nForest_train = accuracy_score(y_train, y_train_pred)\nForest_test = accuracy_score(y_test, y_test_pred)\nprint(f'Forest train/test accuracies '\n      f'{Forest_train:.3f}/{Forest_test:.3f}')\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:57:06.662848Z","iopub.status.idle":"2024-10-27T21:57:06.663394Z","shell.execute_reply.started":"2024-10-27T21:57:06.663095Z","shell.execute_reply":"2024-10-27T21:57:06.663118Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## '''\n\n#Valutation features importances with RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nfeat_labels = train_data[['Pclass','Sex','Age','SibSp','Parch','Fare', 'Title_Miss', 'Title_Mr', 'Title_Mrs', 'Title_Rare']]\nforest = RandomForestClassifier(n_estimators = 500, random_state=1)\nforest.fit(X_train, y_train)\nimportances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]\nprint(\"Feature importances:\")\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f+1, 30, X_train.columns[indices[f]],\n                           importances[indices[f]]))\n\nplt.title('Feature importance')\nplt.bar(range(X_train.shape[1]), importances[indices], align = 'center')\nplt.xticks(range(X_train.shape[1]), X_train.columns[indices], rotation=90)\nplt.xlim([-1,X_train.shape[1]])\nplt.tight_layout()\nplt.show()\n \nX_train_select = X_train.drop(columns = ['SibSp','Parch','Title_Miss','Title_Mrs','Title_Rare','Pclass'])\n#X_train_select = X_train\nX_test_select = X_test.drop(columns = ['SibSp','Parch','Title_Miss','Title_Mrs','Title_Rare','Pclass'])\n#X_test_select = X_test\nX_val = test_data.drop(columns = ['SibSp','Parch','PassengerId', 'Title_Miss','Title_Mrs','Title_Rare','Pclass'])\n#X_val = test_data.drop(columns = ['PassengerId'])\n\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nmodel = xgb.XGBClassifier(n_estimators = 2000, learning_rate = 0.001,\n                         max_depth = 4, random_state = 1,\n                         use_label_encoder = False)\n\ngbm = model.fit(X_train_select, y_train)\ny_train_pred = gbm.predict(X_train_select)\ny_test_pred = gbm.predict(X_test_select)\n\ngbm_train = accuracy_score(y_train, y_train_pred)\ngbm_test = accuracy_score(y_test, y_test_pred)\nprint(f'XGBoost train/test accuracies '\n      f'{gbm_train:.3f}/{gbm_test:.3f}')\n\nimport seaborn as sns\nnew_df = X_train_select.copy()\nnew_df['Survived']=y_train\nnew_df['Predictions']=y_train_pred\nsns.barplot(x='Pclass', y='Predictions', hue = 'Sex',data=new_df)\nplt.show()\nsns.barplot(x='Pclass', y='Survived', hue = 'Sex',data=new_df)\nplt.show()\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-18T21:16:53.287404Z","iopub.execute_input":"2024-10-18T21:16:53.287836Z","iopub.status.idle":"2024-10-18T21:16:53.295520Z","shell.execute_reply.started":"2024-10-18T21:16:53.287799Z","shell.execute_reply":"2024-10-18T21:16:53.294338Z"}}},{"cell_type":"code","source":"'''\npredictions = model.predict(X_val_pca)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:57:06.665081Z","iopub.status.idle":"2024-10-27T21:57:06.665571Z","shell.execute_reply.started":"2024-10-27T21:57:06.665348Z","shell.execute_reply":"2024-10-27T21:57:06.665371Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Algoritmo della divisione in classi sociali\n'''\n#Confronto per classi\n# Filtrare il dataset per classi\nfirst_second_class = train_data[(train_data['Pclass'] == 1) | (train_data['Pclass']==2)]\n#second_class = train_data[train_data['Pclass'] == 2]\nthird_class = train_data[train_data['Pclass'] == 3]\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Barplot del tasso di sopravvivenza per classe\nsns.barplot(x='Pclass', y='Survived', hue = 'Sex', data=train_data)\nplt.title('Tasso di Sopravvivenza per Classe e Sesso')\nplt.xlabel('Classe')\nplt.ylabel('Tasso di Sopravvivenza')\nplt.show()\n\n# Boxplot dell'età per classe e sopravvivenza\nsns.boxplot(x='Pclass', y='Age', hue='Survived', data=train_data)\nplt.title('Distribuzione dell\\'età per Classe e Sopravvivenza')\nplt.xlabel('Classe')\nplt.ylabel('Età')\nplt.show()\n\n# Prima e seconda classe combinate\nfirst_and_second_class = train_data[train_data['Pclass'].isin([1, 2])]\nthird_class = train_data[train_data['Pclass'] == 3]\n\n# Confronto del tasso di sopravvivenza\nprint(f\"Sopravvivenza media - Prima e Seconda Classe: {first_and_second_class['Survived'].mean():.3f}\")\nprint(f\"Sopravvivenza media - Terza Classe: {third_class['Survived'].mean():.3f}\")\n\n# Boxplot del prezzo del biglietto per classe\nsns.boxplot(x='Pclass', y='Fare', data=train_data)\nplt.title('Prezzo del Biglietto per Classe')\nplt.xlabel('Classe')\nplt.ylabel('Prezzo del Biglietto')\nplt.show()\n\nfirst_second_class_tr_data=first_second_class.drop(['Name','Cabin','Embarked','Ticket'],axis=1)\nthird_class_tr_data=third_class.drop(['Name','Cabin','Embarked','Ticket'],axis=1)\ntest_data=test_data.drop(['Name','Cabin','Embarked','Ticket'],axis=1)\nfirst_second_class_test = test_data[(test_data['Pclass'] == 1) | (test_data['Pclass']==2)]\nthird_class_test = test_data[test_data['Pclass'] == 3]\n\ntest_data_sorted = test_data.sort_values(by='PassengerId')\n\n# Supponendo di avere i test_data separati in classi\nfirst_second_class_test_sorted = first_second_class_test.sort_values(by='PassengerId')\nthird_class_test_sorted = third_class_test.sort_values(by='PassengerId')\n\n# Rimettere insieme i test set\ncombined_test_data = pd.concat([first_second_class_test_sorted,  \n                                third_class_test_sorted])\n\n\nX_val_first_second_class = first_second_class_test_sorted.drop(columns = ['PassengerId'])\nX_val_third_class = third_class_test_sorted.drop(columns = ['PassengerId'])\nX_first_second_class = first_second_class_tr_data.drop(columns=['Survived','PassengerId'])\nX_third_class = third_class_tr_data.drop(columns=['Survived','PassengerId'])\nfrom sklearn.model_selection import train_test_split\ny_first_second_class = first_second_class_tr_data['Survived']\ny_third_class = third_class_tr_data['Survived']\nX_f_s_train, X_f_s_test, y_f_s_train, y_f_s_test = train_test_split(X_first_second_class, y_first_second_class, test_size=0.2, stratify=y_first_second_class, random_state=42)\nX_t_train, X_t_test, y_t_train, y_t_test =  train_test_split(X_third_class,y_third_class, test_size=0.2, stratify=y_third_class, random_state=42)\n\n#Applichiamo PCA e Random Forest Classifier alla prima e seconda clase\n#Selection Features through PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n#Stardadize the features\nsc_f_s = StandardScaler()\nX_train_f_s_std = sc_f_s.fit_transform(X_f_s_train)\nX_test_f_s_std = sc_f_s.transform(X_f_s_test)\nX_val_f_s_std = sc_f_s.transform(X_val_first_second_class)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.metrics import accuracy_score\n\nlda_f_s = LDA(n_components=1)\nmodel_f_s = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1, class_weight='balanced')\nX_train_f_s_lda = lda_f_s.fit_transform(X_train_f_s_std, y_f_s_train)\nX_test_f_s_lda = lda_f_s.transform(X_test_f_s_std)\nX_val_f_s_lda = lda_f_s.transform(X_val_f_s_std)\nmodel_f_s.fit(X_train_f_s_lda, y_f_s_train)\ny_train_f_s_pred = model_f_s.predict(X_train_f_s_lda)\ny_test_f_s_pred = model_f_s.predict(X_test_f_s_lda)\nForest_train_f_s = accuracy_score(y_f_s_train, y_train_f_s_pred)\nForest_test_f_s = accuracy_score(y_f_s_test, y_test_f_s_pred)\nprint(f'Forest train First and Second Class/test accuracies '\n      f'{Forest_train_f_s:.3f}/{Forest_test_f_s:.3f}')\n\npredictions_f_s = model_f_s.predict(X_val_f_s_lda)\n\n\n#Applichiamo PCA e Random Forest Classifier alla terza clase\n#Selection Features through PCA\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\n\n#Stardadize the features\nsc_t = StandardScaler()\nX_train_t_std = sc_t.fit_transform(X_t_train)\nX_test_t_std = sc_t.transform(X_t_test)\nX_val_t_std = sc_t.transform(X_val_third_class)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.metrics import accuracy_score\n\nlda_t = LDA(n_components=1)\nmodel_t = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1, class_weight='balanced')\nX_train_t_lda = lda_t.fit_transform(X_train_t_std, y_t_train)\nX_test_t_lda = lda_t.transform(X_test_t_std)\nX_val_t_lda = lda_t.transform(X_val_t_std)\nmodel_t.fit(X_train_t_lda, y_t_train)\ny_train_t_pred = model_t.predict(X_train_t_lda)\ny_test_t_pred = model_t.predict(X_test_t_lda)\nForest_train_t = accuracy_score(y_t_train, y_train_t_pred)\nForest_test_t = accuracy_score(y_t_test, y_test_t_pred)\nprint(f'Forest train third class/test accuracies '\n      f'{Forest_train_t:.3f}/{Forest_test_t:.3f}')\n\npredictions_t = model_t.predict(X_val_t_lda)\n\n# Combinare le predizioni per ogni classe nello stesso ordine\ny_pred_combined = np.concatenate([y_train_f_s_pred, y_train_t_pred])\ny_test_pred_combined = np.concatenate([predictions_f_s, predictions_t])\ny_train = np.concatenate([y_f_s_train, y_t_train])\n\n# Rimettere insieme i dati\ncombined_test_data['Survived'] = y_test_pred_combined\ncombined_test_data_sorted = combined_test_data.sort_values(by='PassengerId')\n\n# Calcolare l'accuratezza globale\nglobal_accuracy = accuracy_score(y_train, y_pred_combined)\nprint(f'Accuratezza Globale: {global_accuracy:.3f}')\n\nsubmission = combined_test_data_sorted[['PassengerId', 'Survived']].sort_values(by='PassengerId')\nsubmission.to_csv('titanic_submission.csv', index=False)\n#output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n#output.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")\n\nprint(submission['PassengerId'].head(10))  # Controlla i primi 10 PassengerId\nprint(test_data['PassengerId'].head(10))   # Confronta con il test_data originale\n\n'''","metadata":{"execution":{"iopub.status.busy":"2024-10-27T21:57:06.668042Z","iopub.status.idle":"2024-10-27T21:57:06.668521Z","shell.execute_reply.started":"2024-10-27T21:57:06.668297Z","shell.execute_reply":"2024-10-27T21:57:06.668320Z"},"trusted":true},"outputs":[],"execution_count":null}]}